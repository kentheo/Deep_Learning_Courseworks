{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 3: Generative Models\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "Please submit on CATe a zip file named *CW3.zip* containing the following:\n",
    "1. A version of this notebook containing your answers. Write your answers in the cells below each question.\n",
    "2. Your trained models as *CAE_model.pth, DCGAN_model_D.pth, DCGAN_model_G.pth*\n",
    "3. You training losses as *train_losses_CAE.npy*, *train_losses_D.npy*, *train_losses_G.npy*\n",
    "\n",
    "#### Working environment:\n",
    "\n",
    "Similarly to the previous coursework, we recommend that you use Google Colaboratory in order to train the required networks.\n",
    "\n",
    "**The deadline for submission is 19:00, Thursday 28th February, 2019**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For this coursework you are asked to implement two commonly used generative models:\n",
    "1. A **Convolutional Autoencoder (CAE)**\n",
    "2. A **Deep Convolutional Generative Adversarial Network (DCGAN)**\n",
    "\n",
    "The dataset you will be using is the CIFAR-10 (https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "## Part 1 (50 points)\n",
    "\n",
    "1. For the CAE, the success of your models will be tested as follows:\n",
    "    - **By the autoencoders' reconstruction error**. You will need to achieve a low enough error in order to reconstruct the images of the dataset with relatively high fidelity. You will have to provide us with your best model's training loss curve, reconstruction error on the test set and some reconstructed images in the respective cells.\n",
    "    - **By the representation learning capabilities of your model**. In particular, autoencoders are known to be able to learn quite informative features in their latent space (embeddings) that can later be used for downstream tasks. In this coursework you are asked to use the representations that your pre-trained encoder yields in order to do image classification. You can use your favourite classification module on top of the features in order to solve the problem *(Hint: a simple Support Vector Machine - SVM - is acceptable. Alternatively, you can devise more complex models such as a Multilayer Fully Connected Network)*.\n",
    " \n",
    "## Part 2 (50 points)\n",
    "2. For the DCGAN, The success of your models will be tested as follows:\n",
    "    - **By the model's training error**. You will need to achieve relatively balanced errors for the generator and the discriminator of your model in order to sample realistic images from the generator. You will have to provide us with your best model's training losses curves, a discussion on how you concluded to the chosen architecture, and visualizations of generated samples in the respective cells. Your results do not have to be perfect, however a good discussion on the choice of architecture will be valued.\n",
    "    - **By avoiding mode collapse**. A common problem of training GANs is that they end up generating only a few different samples (if not only one), rather than learning the whole distribution of the training data. This problem is referred to as mode collapse. You will need to make a discussion on whether you noticed mode collapse or not during your experimentation and if yes, how you addressed it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFEt7wGXP_aE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def denorm(x, channels=None, w=None ,h=None, resize = False):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    if resize:\n",
    "        if channels is None or w is None or h is None:\n",
    "            print('Number of channels, width and height must be provided for resize.')\n",
    "        x = x.view(x.size(0), channels, w, h)\n",
    "    return x\n",
    "\n",
    "def show(img):\n",
    "    if torch.cuda.is_available():\n",
    "        img = img.cpu()\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7chG5g8RmXb"
   },
   "source": [
    "### Device Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zi6L64YuQsLS",
    "outputId": "bcce6ebb-d78d-44b9-affc-3239abc312f5"
   },
   "outputs": [],
   "source": [
    "GPU = True\n",
    "device_idx = 1\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We set a random seed to ensure that your results are reproducible.\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "if not os.path.exists('./CW/CAE'):\n",
    "    os.makedirs('./CW/CAE')\n",
    "if not os.path.exists('./CW/DCGAN'):\n",
    "    os.makedirs('./CW/DCGAN')\n",
    "\n",
    "\n",
    "NUM_TRAIN = 49000\n",
    "\n",
    "transform = transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "data_dir = './datasets'\n",
    "cifar10_train = datasets.CIFAR10(data_dir, train=True, download=True,\n",
    "                             transform=transform)\n",
    "cifar10_val = datasets.CIFAR10(data_dir, train=True, download=True,\n",
    "                           transform=transform)\n",
    "cifar10_test = datasets.CIFAR10(data_dir, train=False, download=True, \n",
    "                            transform=transform)\n",
    "\n",
    "loader_train = DataLoader(cifar10_train, batch_size=batch_size, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "loader_val = DataLoader(cifar10_val, batch_size=batch_size, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "loader_test = DataLoader(cifar10_test, batch_size=batch_size)\n",
    "\n",
    "it = iter(loader_test)\n",
    "sample_inputs, _ = next(it)\n",
    "fixed_input = sample_inputs[0:32, :, :, :]\n",
    "save_image(denorm(fixed_input), './CW/CAE/input_sample.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 (30 points)\n",
    "**Your Task**: \n",
    "\n",
    "a. Implement the CAE architecture. Fill in the missing parts in the cells below in order to complete the CAE class. You will need to define:\n",
    "\n",
    "- The hyperparameters\n",
    "- The constructor\n",
    "- `encode`\n",
    "- `decode`\n",
    "\n",
    "b. Plot your training loss curve (x-axis: epochs, y-axis: loss)\n",
    "\n",
    "c. Calculate the reconstruction error on your test set\n",
    "\n",
    "d. Visualize a subset of the images of the test set and their reconstructions\n",
    "\n",
    "For b., c. and d. the code is already given. Make sure that the version of the notebook you deliver includes these results. \n",
    "\n",
    "Some reccomendations:\n",
    "- add several convolutional layers (3-4).\n",
    "- accelerate training with batch normalization after every convolutional layer or fully connected layer.\n",
    "- use the appropriate activation functions. \n",
    "- Encoder module: hierarchially downsample your images with pooling layers, or strided convolutions.\n",
    "- Decoder module: the upsampling can be done with various methods, such as nearest neighbor upsampling (`torch.nn.Upsample`) or transposed convolutions(`torch.nn.ConvTranspose2d`). \n",
    "\n",
    "Try to follow the common practices for CNNs (e.g small receptive fields, max pooling, RELU activations), in order to narrow down your possible choices. **You will need to choose sufficiently large size for your latent vectors (hidden_size variable), in order to allow enough capacity for your network to represent the data.**\n",
    "\n",
    "The number of epochs that will be needed in order to train the network will vary depending on your choices. In most of the cases, it will be a long procedure (a few hours), so you can leave your notebook running until the training converges.  You don't need to train the network to an extreme if you don't have the time. As an advice, we recommend that while experimenting you should allow around 20 epochs and if the loss doesn't sufficiently drop, restart the training with a more powerful architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *CODE FOR PART 1.1 IN THIS CELL*\n",
    "\n",
    "### Choose the number of epochs and the learning rate.\n",
    "num_epochs = None\n",
    "learning_rate  = None\n",
    "###\n",
    "\n",
    "# Define here other hyperparameters that you used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *CODE FOR PART 1.1 IN THIS CELL*\n",
    "\n",
    "### Choose a value for the latent space dimension and use it in your model\n",
    "hidden_size = None\n",
    "###\n",
    "\n",
    "class CAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAE, self).__init__()\n",
    "        \n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    " \n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################       \n",
    "\n",
    "    def encode(self, x):\n",
    "        \n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    " \n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################   \n",
    "    \n",
    "    def decode(self, z):\n",
    "        \n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    " \n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################      \n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        recon = self.decode(z)\n",
    "        return recon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='mean')  # can we use any other loss here? You are free to choose.\n",
    "def loss_function_CAE(recon_x, x):\n",
    "    recon_loss = criterion(recon_x, x)\n",
    "    return recon_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model and print number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CAE().to(device)\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters is: {}\".format(params))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose and initialize optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are free to add a scheduler or change the optimizer if you want. We chose one for you for simplicity.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(loader_train):\n",
    "        img, _ = data\n",
    "        img = img.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # forward\n",
    "        recon_batch = model(img)\n",
    "        loss = loss_function_CAE(recon_batch, img)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    # print out losses and save reconstructions for every epoch\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, train_loss / len(loader_train)))\n",
    "    recon = model(fixed_input.to(device))\n",
    "    recon = denorm(recon.cpu())\n",
    "    save_image(recon, './CW/CAE/reconstructed_epoch_{}.png'.format(epoch))\n",
    "    train_losses.append(train_loss/ len(loader_train))\n",
    "\n",
    "# save the model and the loss values\n",
    "np.save('./CW/CAE/train_losses.npy', np.array(train_losses))\n",
    "torch.save(model.state_dict(), './CW/CAE/CAE_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_losses = np.load('./CW/CAE/train_losses.npy')\n",
    "plt.plot(list(range(0,train_losses.shape[0])), train_losses)\n",
    "plt.title('Train Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model.load_state_dict(torch.load('./CW/CAE/model.pth'))\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(loader_test):\n",
    "        img,_ = data\n",
    "        img = img.to(device)\n",
    "        recon_batch = model(img)\n",
    "        test_loss += loss_function_CAE(recon_batch, img)\n",
    "    # loss calculated over the whole test set\n",
    "    test_loss /= len(loader_test.dataset)\n",
    "    print('Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set images and reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model.load_state_dict(torch.load('./CW/CAE/CAE_model.pth'))\n",
    "it = iter(loader_test)\n",
    "sample_inputs, _ = next(it)\n",
    "fixed_input = sample_inputs[0:32, :, :, :]\n",
    "\n",
    "# visualize the original images of the last batch of the test set\n",
    "img = make_grid(denorm(fixed_input), nrow=8, padding=2, normalize=False,\n",
    "                range=None, scale_each=False, pad_value=0)\n",
    "show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # visualize the reconstructed images of the last batch of test set\n",
    "    recon_batch = model(fixed_input.to(device)).cpu()\n",
    "    recon_batch = make_grid(denorm(recon_batch), nrow=8, padding=2, normalize=False,\n",
    "                            range=None, scale_each=False, pad_value=0)\n",
    "    show(recon_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. 2 (20 points)\n",
    "**Your Task**: \n",
    "\n",
    "In this part of the exercise you will use your pretrained encoder as a feature extractor in order to solve a downstream task:\n",
    "* For every sample of your training set you will need to extract its latent representation by passing it through the encoder.\n",
    "* Create a classifier of your choice and train it with the extracted features in order to predict the class that each image belongs to. You can access the sample's classes as follows:\n",
    "\n",
    "`it = iter(loader_test)\n",
    "samples, classes = next(it)`\n",
    "* Use the encoder to encode all your test images into latent representations and then use your trained classifier to predict their classes\n",
    "* Print the accuracy of your model.\n",
    "\n",
    "The classifier can be trained with representations that do not yield very accurate reconstructions, so you can stop your training even if the reconstructed images are blurry. Also, note that you do not have to acheive high classification accuracy to get full marks for this question. Instead, focus on describing how you experimented in order to build your best classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *CODE FOR PART 1.2 IN THIS CELL*\n",
    "\n",
    "#######################################################################\n",
    "#                       ** START OF YOUR CODE **\n",
    "#######################################################################\n",
    "\n",
    "#######################################################################\n",
    "#                       ** END OF YOUR CODE **\n",
    "####################################################################### \n",
    "# name your accuracy variable as accuracy\n",
    "print('Classification accuracy: {:.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Deep Convolutional GAN\n",
    "\n",
    "In this task, your main objective is to train a DCGAN (https://arxiv.org/abs/1511.06434) on the CIFAR-10 dataset. You should experiment with different architectures, tricks for stability in training (such as using different activation functions, batch normalization, different values for the hyper-parameters, etc.). In the end, you should provide us with: \n",
    "\n",
    "- your best trained model (which we will be able to run), \n",
    "- some generations for the fixed latent vectors $\\mathbf{z}\\sim \\mathcal{N}\\left(\\mathbf{0}, \\mathbf{I}\\right)$ we have provided you with (train for a number of epochs and make sure there is no mode collapse), \n",
    "- plos with the losses for the discriminator $D$ and the generator $G$ as the training progresses and explain whether your produced plots are theoretically sensible and why this is (or not) the case. \n",
    "- a discussion on whether you noticed any mode collapse, where this behaviour may be attributed to, and explain what you did in order to cope with mode collapse. \n",
    "\n",
    "_Clarification: You should not be worrying too much about getting an \"optimal\" performance on your trained GAN. We want you to demonstrate to us that you experimented with different types of DCGAN variations, report what difficulties transpired throughout the training process, etc. In other words, if we see that you provided us with a running implementation, that you detail different experimentations that you did before providing us with your best one, and that you have grapsed the concepts, you can still get full marks. The attached model does not have to be perfect._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1 (30 points)\n",
    "**Your Task**: \n",
    "\n",
    "a. Implement the DCGAN architecture. Fill in the missing parts in the cells below in order to complete the Generator and Discriminator classes. You will need to define:\n",
    "\n",
    "- The hyperparameters\n",
    "- The constructors\n",
    "- `decode`\n",
    "- `discriminator`\n",
    "\n",
    "b. visualize images sampled from your best model's generator.\n",
    "\n",
    "c. Discuss the experimentations which led to your final architecture. You can plot losses or generated results by other architectures that you tested to back your arguments (but this is not necessary to get full marks).\n",
    "\n",
    "For b. the code is already given. Make sure that the version of the notebook you deliver includes these results. \n",
    "\n",
    "Recomendations for experimentation:\n",
    "- use the architecture that you implemented for the Autoencoder of Part 1 (encoder as discriminator, decoder as generator).\n",
    "- use the architecture desribed in the DCGAN paper (https://arxiv.org/abs/1511.06434).\n",
    "\n",
    "Some general reccomendations:\n",
    "- add several convolutional layers (3-4).\n",
    "- accelerate training with batch normalization after every convolutional layer.\n",
    "- use the appropriate activation functions. \n",
    "- Generator module: the upsampling can be done with various methods, such as nearest neighbor upsampling (`torch.nn.Upsample`) or transposed convolutions(`torch.nn.ConvTranspose2d`). \n",
    "- Discriminator module: Experiment with batch normalization (`torch.nn.BatchNorm2d`) and leaky relu (`torch.nn.LeakyReLu`) units after each convolutional layer.\n",
    "\n",
    "Try to follow the common practices for CNNs (e.g small receptive fields, max pooling, RELU activations), in order to narrow down your possible choices.\n",
    "\n",
    "The number of epochs that will be needed in order to train the network will vary depending on your choices. As an advice, we recommend that while experimenting you should allow around 20 epochs and if the loss doesn't sufficiently drop, restart the training with a more powerful architecture. You don't need to train the network to an extreme if you don't have the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *CODE FOR PART 2.1 IN THIS CELL*\n",
    "\n",
    "### Choose the number of epoch, the learning rate\n",
    "#   and the size of the Generator's input noise vetor.\n",
    "num_epochs = None\n",
    "learning_rate  = None\n",
    "latent_vector_size = None\n",
    "###\n",
    "\n",
    "# Define here other hyperparameters that you used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *CODE FOR PART 2.1 IN THIS CELL*\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "\n",
    "    def decode(self, z):\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "        return x\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decode(z)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "        \n",
    "    def discriminator(self, x):\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        ####################################################################### \n",
    "        \n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.discriminator(x)\n",
    "        return outs.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model and print number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use method `weights_init` to initialize the weights of the Generator and Discriminator networks. Otherwise, implement your own initialization, or do not use at all. You will not be penalized for not using initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_weights_init = True\n",
    "\n",
    "model_G = Generator().to(device)\n",
    "if use_weights_init:\n",
    "    model_G.apply(weights_init)\n",
    "params_G = sum(p.numel() for p in model_G.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters in Generator is: {}\".format(params_G))\n",
    "print(model_G)\n",
    "print('\\n')\n",
    "\n",
    "model_D = Discriminator().to(device)\n",
    "if use_weights_init:\n",
    "    model_D.apply(weights_init)\n",
    "params_D = sum(p.numel() for p in model_D.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters in Discriminator is: {}\".format(params_D))\n",
    "print(model_D)\n",
    "print('\\n')\n",
    "\n",
    "print(\"Total number of parameters is: {}\".format(params_G + params_D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss(reduction='mean')\n",
    "def loss_function(out, label):\n",
    "    loss = criterion(out, label)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose and initialize optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "# You are free to add a scheduler or change the optimizer if you want. We chose one for you for simplicity.\n",
    "beta1 = 0.5\n",
    "optimizerD = torch,optim.Adam(model_D.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(model_G.parameters(), lr=learning_rate, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define fixed input vectors to monitor training and mode collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(batch_size, latent_vector_size, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_folder = './CW/DCGAN'\n",
    "train_losses_G = []\n",
    "train_losses_D = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(loader_train, 0):\n",
    "        train_loss_D = 0\n",
    "        train_loss_G = 0\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        model_D.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label, device=device)\n",
    "\n",
    "        output = model_D(real_cpu)\n",
    "        errD_real = loss_function(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, latent_vector_size, 1, 1, device=device)\n",
    "        fake = model_G(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = model_D(fake.detach())\n",
    "        errD_fake = loss_function(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        train_loss_D += errD.item()\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        model_G.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = model_D(fake)\n",
    "        errG = loss_function(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        train_loss_G += errG.item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, num_epochs, i, len(loader_train),\n",
    "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "    if epoch == 0:\n",
    "        save_image(denorm(real_cpu.cpu()), './CW/DCGAN/real_samples.png')\n",
    "    \n",
    "    fake = model_G(fixed_noise)\n",
    "    save_image(denorm(fake.cpu()), './CW/DCGAN/fake_samples_epoch_%03d.png' % epoch)\n",
    "    train_losses_D.append(train_loss_D / len(loader_train))\n",
    "    train_losses_G.append(train_loss_G / len(loader_train))\n",
    "            \n",
    "# save losses and models\n",
    "np.save(np.array(train_losses_D),'./CW/DCGAN/train_losses_D.npy')\n",
    "np.save(np.array(train_losses_G),'./CW/DCGAN/train_losses_G.npy')\n",
    "torch.save(model_G.state_dict(), './CW/DCGAN/DCGAN_model_G.pth')\n",
    "torch.save(model_D.state_dict(), './CW/DCGAN/DCGAN_model_D.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISCUSS THE SELECTION OF THE ARCHITECTURE IN THIS CELL*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(loader_test)\n",
    "sample_inputs, _ = next(it)\n",
    "fixed_input = sample_inputs[0:32, :, :, :]\n",
    "\n",
    "# visualize the original images of the last batch of the test set\n",
    "img = make_grid(denorm(fixed_input), nrow=4, padding=2, normalize=False,\n",
    "                range=None, scale_each=False, pad_value=0)\n",
    "show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model_G.load_state_dict(torch.load('./CW/DCGAN/DCGAN_model_G.pth'))\n",
    "input_noise = torch.randn(batch_size, latent_vector_size, 1, 1, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # visualize the generated images\n",
    "    generated = model_G(input_noise).cpu()\n",
    "    generated = make_grid(denorm(generated)[:32], nrow=8, padding=2, normalize=False, \n",
    "                        range=None, scale_each=False, pad_value=0)\n",
    "    show(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train losses curves\n",
    "**Your task:**\n",
    "\n",
    "\n",
    "Plot the losses curves for the discriminator $D$ and the generator $G$ as the training progresses and explain whether the produced curves are theoretically sensible and why this is (or not) the case (x-axis: epochs, y-axis: loss).\n",
    "\n",
    "The code for generating the plot is already given. Make sure that the version of the notebook you deliver includes these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_losses_D = np.load('./CW/DCGAN/train_losses_D.npy')\n",
    "train_losses_G = np.load('./CW/DCGAN/train_losses_G.npy')\n",
    "plt.plot(list(range(0,train_losses_D.shape[0])), train_losses_D, label='loss_D')\n",
    "plt.plot(list(range(0,train_losses_G.shape[0])), train_losses_G, label='loss_G')\n",
    "plt.legend()\n",
    "plt.title('Train Losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER FOR PART 2.2 IN THIS CELL*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3 (10 points) \n",
    "**Your task:** \n",
    "\n",
    "Based on the images created by your generator using the `fixed_noise` vector during training, provide a discussion on whether you noticed any mode collapse, where this behaviour may be attributed to, and explain what you did in order to cope with mode collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER FOR PART 2.3 IN THIS CELL*\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CW3-AE/VAE.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
