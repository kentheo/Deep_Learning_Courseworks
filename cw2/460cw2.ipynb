{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "460cw2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "DFNuUV57X7Wb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Coursework2: Convolutional Neural Networks "
      ]
    },
    {
      "metadata": {
        "id": "zs83VbY0X7Wd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## instructions"
      ]
    },
    {
      "metadata": {
        "id": "PfvLCiHAX7We",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Please submit a version of this notebook containing your answers **together with your trained model** on CATe as CW2.zip. Write your answers in the cells below each question.\n",
        "\n",
        "A PDF version of this notebook is also provided in case the figures do not render correctly.\n",
        "\n",
        "**The deadline for submission is 19:00, Thu 14th February, 2019**"
      ]
    },
    {
      "metadata": {
        "id": "e49L7j_WuU6O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Va0IOOQNX7Wf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setting up working environment "
      ]
    },
    {
      "metadata": {
        "id": "fpssV-XAX7Wg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For this coursework you will need to train a large network, therefore we recommend you work with Google Colaboratory, which provides free GPU time. You will need a Google account to do so. \n",
        "\n",
        "Please log in to your account and go to the following page: https://colab.research.google.com. Then upload this notebook.\n",
        "\n",
        "For GPU support, go to \"Edit\" -> \"Notebook Settings\", and select \"Hardware accelerator\" as \"GPU\".\n",
        "\n",
        "You will need to install pytorch by running the following cell:"
      ]
    },
    {
      "metadata": {
        "id": "FUEcmIDXX7Wh",
        "colab_type": "code",
        "outputId": "b3f08eca-b50d-4c57-a773-2fa03168210d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kEfehAyRX7Wl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ]
    },
    {
      "metadata": {
        "id": "3fnp8vV8X7Wm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For this coursework you will implement one of the most commonly used model for image recognition tasks, the Residual Network. The architecture is introduced in 2015 by Kaiming He, et al. in the paper [\"Deep residual learning for image recognition\"](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf). \n",
        "<br>\n",
        "\n",
        "In a residual network, each block contains some convolutional layers, plus \"skip\" connections, which allow the activations to by pass a layer, and then be summed up with the activations of the skipped layer. The image below illustrates a building block in residual networks."
      ]
    },
    {
      "metadata": {
        "id": "Z1ZiCjq_X7Wm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![resnet-block](utils/resnet-block.png)"
      ]
    },
    {
      "metadata": {
        "id": "bvJe5zEoX7Wn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Depending on the number of building blocks, resnets can have different architectures, for example ResNet-50, ResNet-101 and etc. Here you are required to build ResNet-18 to perform classification on the CIFAR-10 dataset, therefore your network will have the following architecture:"
      ]
    },
    {
      "metadata": {
        "id": "NbXyJyYRX7Wo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![resnet](utils/resnet.png)"
      ]
    },
    {
      "metadata": {
        "id": "TBW7Zv-qX7Wp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1 (40 points)"
      ]
    },
    {
      "metadata": {
        "id": "16CgPzWwX7Wq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this part, you will use basic pytorch operations to define the 2D convolution and max pooling operation. "
      ]
    },
    {
      "metadata": {
        "id": "oDnW_jImX7Wr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### YOUR TASK"
      ]
    },
    {
      "metadata": {
        "id": "orx7zavbX7Ws",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- implement the forward pass for Conv2D and MaxPool2D\n",
        "- You can only fill in the parts which are specified as \"YOUR CODE HERE\"\n",
        "- You are **NOT** allowed to use the torch.nn module and the conv2d/maxpooling functions in torch.nn.functional"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "K0JLAagD6X4z",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mqd2RBXCX7Ww",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Conv2D(nn.Module):\n",
        "    \n",
        "    def __init__(self, inchannel, outchannel, kernel_size, stride, padding, bias = True):\n",
        "        \n",
        "        super(Conv2D, self).__init__()\n",
        "        \n",
        "        self.inchannel = inchannel\n",
        "        self.outchannel = outchannel\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        \n",
        "        self.weights = nn.Parameter(torch.Tensor(outchannel, inchannel, \n",
        "                                                 kernel_size, kernel_size))\n",
        "        self.weights.data.normal_(-0.1, 0.1)\n",
        "        \n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(outchannel, ))\n",
        "            self.bias.data.normal_(-0.1, 0.1)\n",
        "        else:\n",
        "            self.bias = None\n",
        "            \n",
        "        \n",
        "    def forward(self, x):\n",
        "        ##############################################################\n",
        "        #                       YOUR CODE HERE                       #       \n",
        "        ##############################################################\n",
        "        \n",
        "        W_out = int((x.shape[2] - self.kernel_size + 2*(self.padding)) / self.stride) + 1\n",
        "        H_out = int((x.shape[3] - self.kernel_size + 2*(self.padding)) / self.stride) + 1\n",
        "        C_out = self.outchannel\n",
        "        \n",
        "        # Unfold the input\n",
        "        x_unf = torch.nn.functional.unfold(x, kernel_size=self.kernel_size, \\\n",
        "                                               stride = self.stride, padding = self.padding)\n",
        "        # Multiply input with weights\n",
        "        output_unf = x_unf.transpose(1, 2).matmul(self.weights.view(self.weights.size(0), -1).t()).transpose(1, 2)\n",
        "        \n",
        "        # View output in proper dimension size for the next layer\n",
        "        output = output_unf.view(x.size(0), self.outchannel, W_out, H_out)  \n",
        "        ##############################################################\n",
        "        #                       END OF YOUR CODE                     #\n",
        "        ##############################################################\n",
        "        \n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NT1usAwfX7Wz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MaxPool2D(nn.Module):\n",
        "    \n",
        "    def __init__(self, pooling_size):\n",
        "        # assume pooling_size = kernel_size = stride\n",
        "        \n",
        "        super(MaxPool2D, self).__init__()\n",
        "        \n",
        "        self.pooling_size = pooling_size\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        ##############################################################\n",
        "        #                       YOUR CODE HERE                       #       \n",
        "        ##############################################################\n",
        "        stride = self.pooling_size\n",
        "        W_out = int((x.shape[2] - self.pooling_size) / stride) + 1\n",
        "        H_out = int((x.shape[3] - self.pooling_size) / stride) + 1\n",
        "        \n",
        "        # Unfold the input\n",
        "        x_unf = torch.nn.functional.unfold(x, kernel_size=self.pooling_size, \\\n",
        "                                               stride = self.pooling_size)\n",
        "        \n",
        "        # Kernel size\n",
        "        w = self.pooling_size ** 2\n",
        "        \n",
        "        # View unfolded input in a beneficial dimension size\n",
        "        x_unf = x_unf.view(x_unf.size(0), x.size(1), w, -1)\n",
        "        \n",
        "        # Get max values from the kernel window\n",
        "        x_unf = x_unf.max(dim=2)[0]\n",
        "        \n",
        "        # View output in proper dimension size for the next layer\n",
        "        output = x_unf.view(x.size(0), x.size(1), W_out, H_out)\n",
        "        ##############################################################\n",
        "        #                       END OF YOUR CODE                     #\n",
        "        ##############################################################\n",
        "                \n",
        "        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "R2bBhs3_6X5H",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define resnet building blocks\n",
        "\n",
        "class ResidualBlock(nn.Module): \n",
        "    def __init__(self, inchannel, outchannel, stride=1): \n",
        "        \n",
        "        super(ResidualBlock, self).__init__() \n",
        "        \n",
        "        self.left = nn.Sequential(Conv2D(inchannel, outchannel, kernel_size=3, \n",
        "                                         stride=stride, padding=1, bias=False), \n",
        "                                  nn.BatchNorm2d(outchannel), \n",
        "                                  nn.ReLU(inplace=True), \n",
        "                                  Conv2D(outchannel, outchannel, kernel_size=3, \n",
        "                                         stride=1, padding=1, bias=False), \n",
        "                                  nn.BatchNorm2d(outchannel)) \n",
        "        \n",
        "        self.shortcut = nn.Sequential() \n",
        "        \n",
        "        if stride != 1 or inchannel != outchannel: \n",
        "            \n",
        "            self.shortcut = nn.Sequential(Conv2D(inchannel, outchannel, \n",
        "                                                 kernel_size=1, stride=stride, \n",
        "                                                 padding = 0, bias=False), \n",
        "                                          nn.BatchNorm2d(outchannel) ) \n",
        "            \n",
        "    def forward(self, x): \n",
        "        \n",
        "        out = self.left(x) \n",
        "        \n",
        "        out += self.shortcut(x) \n",
        "        \n",
        "        out = F.relu(out) \n",
        "        \n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U-Sa0BAw6X5P",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define resnet\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, ResidualBlock, num_classes = 10):\n",
        "        \n",
        "        super(ResNet, self).__init__()\n",
        "        \n",
        "        self.inchannel = 64\n",
        "        self.conv1 = nn.Sequential(Conv2D(3, 64, kernel_size = 3, stride = 1,\n",
        "                                            padding = 1, bias = False), \n",
        "                                  nn.BatchNorm2d(64), \n",
        "                                  nn.ReLU())\n",
        "        \n",
        "        self.layer1 = self.make_layer(ResidualBlock, 64, 2, stride = 1)\n",
        "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride = 2)\n",
        "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride = 2)\n",
        "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride = 2)\n",
        "        self.maxpool = MaxPool2D(4)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "        \n",
        "    \n",
        "    def make_layer(self, block, channels, num_blocks, stride):\n",
        "        \n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        \n",
        "        layers = []\n",
        "        \n",
        "        for stride in strides:\n",
        "            \n",
        "            layers.append(block(self.inchannel, channels, stride))\n",
        "            \n",
        "            self.inchannel = channels\n",
        "            \n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        x = self.maxpool(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    \n",
        "def ResNet18():\n",
        "    return ResNet(ResidualBlock)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GJl5PAf0X7W9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2 (40 points)"
      ]
    },
    {
      "metadata": {
        "id": "KjZmX9JrX7W-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this part, you will train the ResNet-18 defined in the previous part on the CIFAR-10 dataset. Code for loading the dataset, training and evaluation are provided. "
      ]
    },
    {
      "metadata": {
        "id": "qaFN4ZTRX7XA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Your Task"
      ]
    },
    {
      "metadata": {
        "id": "ly-vA-xnX7XB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Train your network to achieve the best possible test set accuracy after a maximum of 10 epochs of training.\n",
        "\n",
        "2. You can use techniques such as optimal hyper-parameter searching, data pre-processing\n",
        "\n",
        "3. If necessary, you can also use another optimiser\n",
        "\n",
        "4. **Answer the following question:**\n",
        "Given such a network with a large number of trainable parameters, and a training set of a large number of data, what do you think is the best strategy for hyperparameter searching? "
      ]
    },
    {
      "metadata": {
        "id": "FLN94KmXX7XE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**YOUR ANSWER FOR 2.4 HERE**\n",
        "\n",
        "A:\n",
        "Various strategies can be used for hyperparameter searching. One of them is Manual Tuning, where we would have to use a set of hyperparameters, observe the performance of the network and then tune them. This is solely based on our empirical knowledge and the knowledge of the dataset.\n",
        "\n",
        "Another naive strategy is Grid Search, where we choose a range for all hyperparameters and loop over the possible configurations, in search of the best combination, i.e the one yielding the best results. This is very inefficient.\n",
        "\n",
        "A less costly solution would be Random Search, where random values are sampled for the hyperparameters given a \"hypersphere\" with certain radius from the currently chosen hyperparameters. This is done until a certain criterion is met (iterations or acceptable performance results).\n",
        "\n",
        "A good strategy would be Bayesian Optimisation, where a Gaussian process will learn the mapping from the hyperparameters combinations to the performance metric used to evaluate the system."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jx4EjUPa6X5d",
        "outputId": "80c80297-4517-48e3-d293-07c752a3b58c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "transform = T.ToTensor()\n",
        "\n",
        "\n",
        "# load data\n",
        "\n",
        "NUM_TRAIN = 49000\n",
        "print_every = 100\n",
        "\n",
        "\n",
        "data_dir = './data'\n",
        "cifar10_train = dset.CIFAR10(data_dir, train=True, download=True, transform=transform)\n",
        "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
        "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
        "\n",
        "cifar10_val = dset.CIFAR10(data_dir, train=True, download=True, transform=transform)\n",
        "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
        "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
        "\n",
        "cifar10_test = dset.CIFAR10(data_dir, train=False, download=True, transform=transform)\n",
        "loader_test = DataLoader(cifar10_test, batch_size=64)\n",
        "\n",
        "\n",
        "USE_GPU = True\n",
        "dtype = torch.float32 \n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Lcscd7k66X5s",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def check_accuracy(loader, model):\n",
        "    # function for test accuracy on validation and test set\n",
        "    \n",
        "    if loader.dataset.train:\n",
        "        print('Checking accuracy on validation set')\n",
        "    else:\n",
        "        print('Checking accuracy on test set')   \n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "            scores = model(x)\n",
        "            _, preds = scores.max(1)\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_samples += preds.size(0)\n",
        "        acc = float(num_correct) / num_samples\n",
        "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "\n",
        "\n",
        "def train_part(model, optimizer, epochs=1):\n",
        "    \"\"\"\n",
        "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
        "    \n",
        "    Inputs:\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "    \n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    for e in range(epochs):\n",
        "#         print(len(loader_train))\n",
        "        for t, (x, y) in enumerate(loader_train):\n",
        "            model.train()  # put model to training mode\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "\n",
        "            scores = model(x)\n",
        "            loss = F.cross_entropy(scores, y)\n",
        "\n",
        "            # Zero out all of the gradients for the variables which the optimizer\n",
        "            # will update.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the parameters of the model using the gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            if t % print_every == 0:\n",
        "                print('Epoch: %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
        "                check_accuracy(loader_val, model)\n",
        "                print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "no8o-2VS6X5y",
        "scrolled": false,
        "outputId": "9d585741-14a1-44d2-88f8-c5514ecc8a7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3959
        }
      },
      "cell_type": "code",
      "source": [
        "# code for optimising your network performance\n",
        "\n",
        "##############################################################\n",
        "#                       YOUR CODE HERE                       #       \n",
        "##############################################################\n",
        "# Normalize data and do other transformations\n",
        "\n",
        "transforms_1 = T.Compose([T.RandomCrop(32, padding=4),\n",
        "                          T.RandomHorizontalFlip(),\n",
        "                          T.ToTensor(), \n",
        "                          T.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "\n",
        "cifar10_train = dset.CIFAR10(data_dir, train=True, download=True, transform=transforms_1)\n",
        "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
        "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
        "\n",
        "cifar10_val = dset.CIFAR10(data_dir, train=True, download=True, transform=transforms_1)\n",
        "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
        "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
        "\n",
        "cifar10_test = dset.CIFAR10(data_dir, train=False, download=True, transform=transforms_1)\n",
        "loader_test = DataLoader(cifar10_test, batch_size=64)\n",
        "\n",
        "##############################################################\n",
        "#                       END OF YOUR CODE                     #\n",
        "##############################################################\n",
        "\n",
        "\n",
        "# define and train the network\n",
        "model = ResNet18()\n",
        "# Optimiser - Change Learning rate, Regularisation\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.003, weight_decay = 0.0000038)\n",
        "train_part(model, optimizer, epochs = 10)\n",
        "\n",
        "\n",
        "# report test set accuracy\n",
        "\n",
        "check_accuracy(loader_test, model)\n",
        "\n",
        "\n",
        "# save the model\n",
        "torch.save(model.state_dict(), 'model.pt')\n",
        "\n",
        "# More test runs were made before this, but we didn't record them as they were\n",
        "# unsatisfactory. After figuring a good learning rate at 0.003, we modify the\n",
        "# weight decay to try to increase the accuracy\n",
        "# Learning rate   |  Weight Decay   |   Accuracy\n",
        "#     0.003          0.0000045           82.55\n",
        "#     0.003          0.000004            83.75\n",
        "#     0.003          0.0000035           83.29\n",
        "#     0.003          0.0000042           83.49\n",
        "#     0.003          0.0000038           84.01\n",
        "#     0.003          0.00000385          83.69"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch: 0, Iteration 0, loss = 2.6842\n",
            "Checking accuracy on validation set\n",
            "Got 90 / 1000 correct (9.00)\n",
            "\n",
            "Epoch: 0, Iteration 100, loss = 2.0768\n",
            "Checking accuracy on validation set\n",
            "Got 181 / 1000 correct (18.10)\n",
            "\n",
            "Epoch: 0, Iteration 200, loss = 2.0792\n",
            "Checking accuracy on validation set\n",
            "Got 216 / 1000 correct (21.60)\n",
            "\n",
            "Epoch: 0, Iteration 300, loss = 1.8175\n",
            "Checking accuracy on validation set\n",
            "Got 236 / 1000 correct (23.60)\n",
            "\n",
            "Epoch: 0, Iteration 400, loss = 1.8259\n",
            "Checking accuracy on validation set\n",
            "Got 215 / 1000 correct (21.50)\n",
            "\n",
            "Epoch: 0, Iteration 500, loss = 1.9387\n",
            "Checking accuracy on validation set\n",
            "Got 321 / 1000 correct (32.10)\n",
            "\n",
            "Epoch: 0, Iteration 600, loss = 1.5631\n",
            "Checking accuracy on validation set\n",
            "Got 386 / 1000 correct (38.60)\n",
            "\n",
            "Epoch: 0, Iteration 700, loss = 1.8573\n",
            "Checking accuracy on validation set\n",
            "Got 378 / 1000 correct (37.80)\n",
            "\n",
            "Epoch: 1, Iteration 0, loss = 1.5286\n",
            "Checking accuracy on validation set\n",
            "Got 426 / 1000 correct (42.60)\n",
            "\n",
            "Epoch: 1, Iteration 100, loss = 1.6426\n",
            "Checking accuracy on validation set\n",
            "Got 413 / 1000 correct (41.30)\n",
            "\n",
            "Epoch: 1, Iteration 200, loss = 1.3070\n",
            "Checking accuracy on validation set\n",
            "Got 481 / 1000 correct (48.10)\n",
            "\n",
            "Epoch: 1, Iteration 300, loss = 1.5268\n",
            "Checking accuracy on validation set\n",
            "Got 463 / 1000 correct (46.30)\n",
            "\n",
            "Epoch: 1, Iteration 400, loss = 1.5279\n",
            "Checking accuracy on validation set\n",
            "Got 506 / 1000 correct (50.60)\n",
            "\n",
            "Epoch: 1, Iteration 500, loss = 1.1081\n",
            "Checking accuracy on validation set\n",
            "Got 491 / 1000 correct (49.10)\n",
            "\n",
            "Epoch: 1, Iteration 600, loss = 1.4506\n",
            "Checking accuracy on validation set\n",
            "Got 534 / 1000 correct (53.40)\n",
            "\n",
            "Epoch: 1, Iteration 700, loss = 0.9741\n",
            "Checking accuracy on validation set\n",
            "Got 522 / 1000 correct (52.20)\n",
            "\n",
            "Epoch: 2, Iteration 0, loss = 1.3874\n",
            "Checking accuracy on validation set\n",
            "Got 588 / 1000 correct (58.80)\n",
            "\n",
            "Epoch: 2, Iteration 100, loss = 1.0620\n",
            "Checking accuracy on validation set\n",
            "Got 590 / 1000 correct (59.00)\n",
            "\n",
            "Epoch: 2, Iteration 200, loss = 1.2100\n",
            "Checking accuracy on validation set\n",
            "Got 595 / 1000 correct (59.50)\n",
            "\n",
            "Epoch: 2, Iteration 300, loss = 0.8434\n",
            "Checking accuracy on validation set\n",
            "Got 630 / 1000 correct (63.00)\n",
            "\n",
            "Epoch: 2, Iteration 400, loss = 1.0713\n",
            "Checking accuracy on validation set\n",
            "Got 611 / 1000 correct (61.10)\n",
            "\n",
            "Epoch: 2, Iteration 500, loss = 0.9234\n",
            "Checking accuracy on validation set\n",
            "Got 617 / 1000 correct (61.70)\n",
            "\n",
            "Epoch: 2, Iteration 600, loss = 0.8476\n",
            "Checking accuracy on validation set\n",
            "Got 667 / 1000 correct (66.70)\n",
            "\n",
            "Epoch: 2, Iteration 700, loss = 0.8236\n",
            "Checking accuracy on validation set\n",
            "Got 690 / 1000 correct (69.00)\n",
            "\n",
            "Epoch: 3, Iteration 0, loss = 0.8522\n",
            "Checking accuracy on validation set\n",
            "Got 639 / 1000 correct (63.90)\n",
            "\n",
            "Epoch: 3, Iteration 100, loss = 0.9389\n",
            "Checking accuracy on validation set\n",
            "Got 672 / 1000 correct (67.20)\n",
            "\n",
            "Epoch: 3, Iteration 200, loss = 0.8812\n",
            "Checking accuracy on validation set\n",
            "Got 667 / 1000 correct (66.70)\n",
            "\n",
            "Epoch: 3, Iteration 300, loss = 0.5864\n",
            "Checking accuracy on validation set\n",
            "Got 675 / 1000 correct (67.50)\n",
            "\n",
            "Epoch: 3, Iteration 400, loss = 0.8306\n",
            "Checking accuracy on validation set\n",
            "Got 707 / 1000 correct (70.70)\n",
            "\n",
            "Epoch: 3, Iteration 500, loss = 0.9536\n",
            "Checking accuracy on validation set\n",
            "Got 729 / 1000 correct (72.90)\n",
            "\n",
            "Epoch: 3, Iteration 600, loss = 0.5471\n",
            "Checking accuracy on validation set\n",
            "Got 727 / 1000 correct (72.70)\n",
            "\n",
            "Epoch: 3, Iteration 700, loss = 0.6093\n",
            "Checking accuracy on validation set\n",
            "Got 710 / 1000 correct (71.00)\n",
            "\n",
            "Epoch: 4, Iteration 0, loss = 0.6495\n",
            "Checking accuracy on validation set\n",
            "Got 727 / 1000 correct (72.70)\n",
            "\n",
            "Epoch: 4, Iteration 100, loss = 0.6261\n",
            "Checking accuracy on validation set\n",
            "Got 754 / 1000 correct (75.40)\n",
            "\n",
            "Epoch: 4, Iteration 200, loss = 0.5935\n",
            "Checking accuracy on validation set\n",
            "Got 736 / 1000 correct (73.60)\n",
            "\n",
            "Epoch: 4, Iteration 300, loss = 0.7432\n",
            "Checking accuracy on validation set\n",
            "Got 768 / 1000 correct (76.80)\n",
            "\n",
            "Epoch: 4, Iteration 400, loss = 0.8378\n",
            "Checking accuracy on validation set\n",
            "Got 741 / 1000 correct (74.10)\n",
            "\n",
            "Epoch: 4, Iteration 500, loss = 0.6430\n",
            "Checking accuracy on validation set\n",
            "Got 736 / 1000 correct (73.60)\n",
            "\n",
            "Epoch: 4, Iteration 600, loss = 0.6464\n",
            "Checking accuracy on validation set\n",
            "Got 739 / 1000 correct (73.90)\n",
            "\n",
            "Epoch: 4, Iteration 700, loss = 0.7855\n",
            "Checking accuracy on validation set\n",
            "Got 783 / 1000 correct (78.30)\n",
            "\n",
            "Epoch: 5, Iteration 0, loss = 0.4979\n",
            "Checking accuracy on validation set\n",
            "Got 779 / 1000 correct (77.90)\n",
            "\n",
            "Epoch: 5, Iteration 100, loss = 0.3710\n",
            "Checking accuracy on validation set\n",
            "Got 781 / 1000 correct (78.10)\n",
            "\n",
            "Epoch: 5, Iteration 200, loss = 0.5171\n",
            "Checking accuracy on validation set\n",
            "Got 778 / 1000 correct (77.80)\n",
            "\n",
            "Epoch: 5, Iteration 300, loss = 0.6076\n",
            "Checking accuracy on validation set\n",
            "Got 783 / 1000 correct (78.30)\n",
            "\n",
            "Epoch: 5, Iteration 400, loss = 0.6061\n",
            "Checking accuracy on validation set\n",
            "Got 794 / 1000 correct (79.40)\n",
            "\n",
            "Epoch: 5, Iteration 500, loss = 0.6556\n",
            "Checking accuracy on validation set\n",
            "Got 793 / 1000 correct (79.30)\n",
            "\n",
            "Epoch: 5, Iteration 600, loss = 0.6634\n",
            "Checking accuracy on validation set\n",
            "Got 804 / 1000 correct (80.40)\n",
            "\n",
            "Epoch: 5, Iteration 700, loss = 0.6280\n",
            "Checking accuracy on validation set\n",
            "Got 769 / 1000 correct (76.90)\n",
            "\n",
            "Epoch: 6, Iteration 0, loss = 0.4633\n",
            "Checking accuracy on validation set\n",
            "Got 806 / 1000 correct (80.60)\n",
            "\n",
            "Epoch: 6, Iteration 100, loss = 0.4767\n",
            "Checking accuracy on validation set\n",
            "Got 780 / 1000 correct (78.00)\n",
            "\n",
            "Epoch: 6, Iteration 200, loss = 0.5614\n",
            "Checking accuracy on validation set\n",
            "Got 799 / 1000 correct (79.90)\n",
            "\n",
            "Epoch: 6, Iteration 300, loss = 0.4108\n",
            "Checking accuracy on validation set\n",
            "Got 808 / 1000 correct (80.80)\n",
            "\n",
            "Epoch: 6, Iteration 400, loss = 0.6138\n",
            "Checking accuracy on validation set\n",
            "Got 806 / 1000 correct (80.60)\n",
            "\n",
            "Epoch: 6, Iteration 500, loss = 0.6367\n",
            "Checking accuracy on validation set\n",
            "Got 795 / 1000 correct (79.50)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "A4tlP5xvDhM9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Part 3 (20 points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VMIqUpkSX7XT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The code provided below will allow you to visualise the feature maps computed by different layers of your network. Run the code (install matplotlib if necessary) and **answer the following questions**: \n",
        "\n",
        "1. Compare the feature maps from low-level layers to high-level layers, what do you observe? \n",
        "\n",
        "2. Use the training log, reported test set accuracy and the feature maps, analyse the performance of your network. If you think the performance is sufficiently good, explain why; if not, what might be the problem and how can you improve the performance?\n",
        "\n",
        "3. What are the other possible ways to analyse the performance of your network?"
      ]
    },
    {
      "metadata": {
        "id": "OPWd9gh0X7XU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**YOUR ANSWER FOR PART 3 HERE**\n",
        "\n",
        "A:\n",
        "1.Feature maps \"capture\" different information in the data.  Neurons in lower layers detect simple features, e.g doing edge detection or finding other more generic patterns. Neurons in middle layers detect parts of an object and learn some localisation, and neurons in higher layers detect concepts (adding semantic value). The output of the higher layers is not very clear to see in the printed output below, but the \"condensed\" pixels hide lots of useful information for the network to process and get results.\n",
        "\n",
        "2.The network has a performance of ~84%. We notice that during training, the accuracy on the validation set reaches and exceeds  70% in 3 epochs. The accuracy reaches 80% in epoch 6 and after that it fluctuates. This percentage of accuracy is not bad for such a training set, where there are some classes that can be hard to be distinguished (e.g.small dogs and cats, horses and deer). Their size compared to the backrgound, for example, along with their similar shape and colour could \"fool\" the network.\n",
        " \n",
        "My approach towards tackling the task at hand was to manually tune the hyperparameters of learning rate and weight penalty of the optimiser.  Also, we tested the performance of various optimisers, like SGD and a few variations of Adam. Adam optimiser produced the best results. We also do some transformations on the data before training.  We used horizontal flipping such that objects and their mirrored view are recognised. We also used random cropping such that the classifier learns parts of objects and not specific features of the objects as a whole in case those are not present in all instances.  Finally, we normalised the data using values specific to the dataset. Doing that we center the data around 0 and see how many standard deviations away each input is from 0. This is necessary, since the different channels/features might come from different ranges of distributions and when multiplying with all weights and the learning rate, they might produce unwanted results (over and undercompensation). \n",
        " \n",
        "The performance of the network can be improved. We are already doing a couple of stuff to improve performance, including normalisation, shuffle and batch training, regularisation and by default use momentum adjustment as Adam is our choice of optimiser. Also, the training set is balanced, meaning there are 10 classes with equal number of instances.\n",
        "One way to improve the performance of the network is to get a larger training set. This can be done by doing data augmentation, i.e.do shifts, rotations and distortions to existing images and adding them to the training set. Another way is to use a scheduler for the learning rate. Instead of having a fixed learning rate, we can introduce a scheduler where the learning rate decays over iterations.\n",
        "\n",
        "3.One way of analysing the performance of the network is by producing more debugging information during training. Instead of only showing the validation accuracy and loss, we could also show a small overview of gradients during training. We could also show the relative changes of weights. This might become ugly due to the large amount of information to be printed and will be hard to go through, especially if the number of epochs is a lot.\n",
        "Another way is to figure out what some types of results clusters of neurons are producing and modify their location in the network. If you find a neuron that does a certain job, you may choose to move it around for better accuracy. This requires some previous knowledge about the network and the current results, in order to fiddle around with it."
      ]
    },
    {
      "metadata": {
        "id": "PviJbi7YX7XV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "vis_labels = ['conv1', 'layer1', 'layer2', 'layer3', 'layer4']\n",
        "\n",
        "for l in vis_labels:\n",
        "\n",
        "    getattr(model, l).register_forward_hook(get_activation(l))\n",
        "    \n",
        "    \n",
        "data, _ = cifar10_test[0]\n",
        "data = data.unsqueeze_(0).to(device = device, dtype = dtype)\n",
        "\n",
        "output = model(data)\n",
        "\n",
        "\n",
        "for idx, l in enumerate(vis_labels):\n",
        "\n",
        "    act = activation[l].squeeze()\n",
        "\n",
        "    if idx < 2:\n",
        "        ncols = 8\n",
        "    else:\n",
        "        ncols = 32\n",
        "        \n",
        "    nrows = act.size(0) // ncols\n",
        "    \n",
        "    fig, axarr = plt.subplots(nrows, ncols)\n",
        "    fig.suptitle(l)\n",
        "\n",
        "\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            axarr[i, j].imshow(act[i * nrows + j].cpu())\n",
        "            axarr[i, j].axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zNFGC3PRX7Xc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**=============== END OF CW2 ===============**"
      ]
    },
    {
      "metadata": {
        "id": "YVYbklq-38I-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}